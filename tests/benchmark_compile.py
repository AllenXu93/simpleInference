import torch
import time
from engine.load import load_model

def benchmark_model(model, input_ids, device, warmup=10, iters=50):
    model.eval()
    model.to(device)
    input_ids = input_ids.to(device)

    # warmup
    for _ in range(warmup):
        with torch.no_grad():
            _ = model(input_ids)

    torch.cuda.synchronize()
    start = time.time()
    for _ in range(iters):
        with torch.no_grad():
            _ = model(input_ids)
    torch.cuda.synchronize()
    end = time.time()

    avg_latency = (end - start) * 1000 / iters  # ms
    return avg_latency


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model_name = "gpt2"
    tokenizer, model, max_length = load_model(model_name)

    input_ids = tokenizer("Once upon a time", return_tensors="pt").input_ids

    print("ğŸš€ åŠ è½½æœªä¼˜åŒ–æ¨¡å‹")
    baseline_time = benchmark_model(model, input_ids, device)
    print(f"ğŸ•’ åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´: {baseline_time:.2f} ms")

    print("âš™ï¸  torch.compile ä¼˜åŒ–ä¸­...")
    compiled_model = torch.compile(model)
    compiled_time = benchmark_model(compiled_model, input_ids, device)
    print(f"ğŸš€ ç¼–è¯‘æ¨¡å‹æ¨ç†æ—¶é—´: {compiled_time:.2f} ms")

    speedup = baseline_time / compiled_time
    print(f"âš¡ï¸ æ¨ç†åŠ é€Ÿæ¯”: {speedup:.2f}x")

if __name__ == "__main__":
    main()
